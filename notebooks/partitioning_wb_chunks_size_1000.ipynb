{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Partition Water Bodies\n",
    "Shapefiles for water bodies are available in <i>NHD</i> (https://nhd.usgs.gov/data.html) and are accessible by state. There are roughly about 100,000 water bodies in each state. Partitioning this large data into chunks eases the processing of it.<br>\n",
    "Shapefiles of <i>states of interest</i> are downloaded from here and are stored in the path <i>NHD_High_Resolution/NHD_state</i> locally. <br>\n",
    "\n",
    "This script partitions water bodies of a given <i>state of interest</i> into chunks of size at most <b>1000</b>. <br>\n",
    "The <i>dbf</i> file containing data of water bodies for the given state are partitioned into <i>pandas dataframe</i> chunks of size at most 1000. \n",
    "Each chunk (n) is stored in folder <i>Given_data_path/state_Lakes_n/lakes.csv</i>.<br>\n",
    "For example, if the given data path is <i>Partitioned-DFS-WI</i> and the state is <i>Wisconsin</i>.\n",
    "The 1st chunk gets stored in <i>Partitioned-DFS-WI/Wisconsin_Lakes_0/lakes.csv</i> locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing node 'org'\n",
      "doing node 'station'\n",
      "doing node 'result'\n",
      "doing node 'activity'\n"
     ]
    }
   ],
   "source": [
    "import shapefile\n",
    "import pandas as pd\n",
    "from shapely.geometry import *\n",
    "from simpledbf import Dbf5\n",
    "from pywqp import pywqp_client\n",
    "from geopandas import *\n",
    "import os,glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition\n",
    "\n",
    "1. Function to partition the dataframe of water bodies into ith chunk\n",
    "2. Function to partition the dataframe of water bodies of given state into chunks of at most size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function to partition the dataframe of water bodies into ith chunk\n",
    "\n",
    "Below is the definition of the function that partitions the water bodies dataframe into ith chunk for a given state\n",
    "and stores the partitioned chunk and details like the <i>dbf path</i>, <i>shape path</i>, <i>start</i>\n",
    "and <i>end indices</i> in file info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_df_i(i,state):\n",
    "    \"\"\"\n",
    "        Partitions the water bodies dataframe into ith chunk for the given state\n",
    "        and stores the dbf path, shape path, start and end indices in file info.csv\n",
    "        Args:\n",
    "            i (str): Input String ,state (str): Input String \n",
    "    \"\"\"\n",
    "    # Global Variables\n",
    "    global directory_wb,data_path,df_wb\n",
    "    \n",
    "    # Size of the chunk\n",
    "    n=1000\n",
    "    \n",
    "    # Path to store the chunk i\n",
    "    directory_batch = directory_wb+'/'+state+'_Lakes_'+str(i/1000)\n",
    "    \n",
    "    # Form the ith chunk and store it in folder path locally\n",
    "    if os.path.exists(directory_batch): \n",
    "        shutil.rmtree(directory_batch)\n",
    "    os.makedirs(directory_batch)\n",
    "    df_wb[i:i + n].to_csv(directory_batch+'/lakes.csv')\n",
    "    \n",
    "    # Store the path of the dbf file, path of shape file, start and end indices\n",
    "    # of the partitioned chunk in file info.csv\n",
    "    cols = ['dbf_path','shape_path','start_index','end_index']\n",
    "    df=pd.DataFrame(columns=cols)\n",
    "    if i+n > len(df_wb):\n",
    "        end_index = len(df_wb)-1\n",
    "    else:\n",
    "        end_index = i+n-1\n",
    "    df.loc[len(df)] = [data_path+'NHDWaterbody.dbf',data_path+'NHDWaterbody.shx',i,end_index]\n",
    "    df.to_csv(directory_batch+'/info.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Function to partition the dataframe of water bodies of given state into chunks of at most size 1000\n",
    "\n",
    "Below is the definition of the function that partitions the water bodies dataframe into chunks of size at most 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partition_df(directory_wb,state):\n",
    "    \"\"\"\n",
    "        Partitions the water bodies dataframe into chunks of size\n",
    "        at most 1000\n",
    "        Args:\n",
    "            directory_wb (str): Input String ,state (str): Input String \n",
    "    \"\"\"\n",
    "    # Local data path to the shape files \n",
    "    data_path = \"NHD_High_Resolution/NHD_\"+state+\"/Shape/\"\n",
    "    # Reading in the .dbf file and converting to pandas dataframe\n",
    "    dbf = Dbf5(data_path+\"NHDWaterbody.dbf\")\n",
    "    df_wb = dbf.to_dataframe()\n",
    "    \n",
    "    # Creating directory to store partitioned data\n",
    "    if os.path.exists(directory_wb): \n",
    "        shutil.rmtree(directory_wb)\n",
    "    os.makedirs(directory_wb)\n",
    "    \n",
    "    [partition_df(i,state) for i in range(0,len(df_wb),1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
